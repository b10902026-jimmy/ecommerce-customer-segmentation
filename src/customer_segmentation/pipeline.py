"""
å®¢æˆ¶åˆ†ç¾¤åˆ†æç®¡é“ - Customer Segmentation Analysis Pipeline

æä¾›å®Œæ•´çš„åˆ†æç®¡é“ï¼Œæ•´åˆæ‰€æœ‰æ¨¡çµ„åŠŸèƒ½ã€‚
"""

from pathlib import Path
from typing import Dict, Optional, Union

import pandas as pd
from loguru import logger

from customer_segmentation.analysis.rfm_calculator import RFMCalculator
from customer_segmentation.data.cleaner import DataCleaner
from customer_segmentation.data.loader import DataLoader
from customer_segmentation.utils.config import get_config
from customer_segmentation.utils.logger import LoggerMixin, log_execution_time
from customer_segmentation.visualization.visualizer import DataVisualizer


class CustomerSegmentationPipeline(LoggerMixin):
    """
    å®¢æˆ¶åˆ†ç¾¤åˆ†æç®¡é“
    
    æ•´åˆè³‡æ–™è¼‰å…¥ã€æ¸…ç†ã€RFM åˆ†æã€åˆ†ç¾¤å’Œè¦–è¦ºåŒ–çš„å®Œæ•´æµç¨‹ã€‚
    """
    
    def __init__(self, config_override: Optional[Dict] = None):
        """
        åˆå§‹åŒ–åˆ†æç®¡é“
        
        Args:
            config_override: è¦†è“‹é è¨­é…ç½®çš„å­—å…¸
        """
        self.config = get_config()
        if config_override:
            for key, value in config_override.items():
                if hasattr(self.config, key):
                    setattr(self.config, key, value)
        
        self.loader = None
        self.cleaner = None
        self.rfm_calculator = None
        self.visualizer = None
        
        # è³‡æ–™å„²å­˜
        self.raw_data = None
        self.cleaned_data = None
        self.rfm_data = None
        self.segmented_data = None
        
        self.logger.info("ğŸ”§ å®¢æˆ¶åˆ†ç¾¤åˆ†æç®¡é“åˆå§‹åŒ–å®Œæˆ Pipeline initialized")
    
    @log_execution_time
    def load_data(self, file_path: Union[str, Path]) -> pd.DataFrame:
        """
        è¼‰å…¥è³‡æ–™
        
        Args:
            file_path: è³‡æ–™æª”æ¡ˆè·¯å¾‘
            
        Returns:
            pd.DataFrame: è¼‰å…¥çš„è³‡æ–™
        """
        self.logger.info(f"ğŸ“‚ è¼‰å…¥è³‡æ–™ Loading data from: {file_path}")
        
        self.loader = DataLoader(str(file_path))
        self.raw_data = self.loader.load_data()
        
        # é©—è­‰å¿…è¦æ¬„ä½
        required_columns = [
            'InvoiceNo', 'StockCode', 'Description', 'Quantity',
            'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country'
        ]
        
        if not self.loader.validate_columns(required_columns):
            raise ValueError("è³‡æ–™ç¼ºå°‘å¿…è¦æ¬„ä½ï¼Œç„¡æ³•é€²è¡Œåˆ†æ")
        
        self.logger.info(f"âœ… è³‡æ–™è¼‰å…¥å®Œæˆ Data loaded: {self.raw_data.shape}")
        return self.raw_data
    
    @log_execution_time
    def clean_data(self, remove_outliers: bool = False) -> pd.DataFrame:
        """
        æ¸…ç†è³‡æ–™
        
        Args:
            remove_outliers: æ˜¯å¦ç§»é™¤ç•°å¸¸å€¼
            
        Returns:
            pd.DataFrame: æ¸…ç†å¾Œçš„è³‡æ–™
        """
        if self.raw_data is None:
            raise ValueError("è«‹å…ˆè¼‰å…¥è³‡æ–™ Please load data first")
        
        self.logger.info("ğŸ§¹ é–‹å§‹è³‡æ–™æ¸…ç† Starting data cleaning")
        
        self.cleaner = DataCleaner(self.raw_data)
        self.cleaned_data = self.cleaner.clean_all(remove_outliers=remove_outliers)
        
        # è¨˜éŒ„æ¸…ç†çµæœ
        cleaning_summary = self.cleaner.get_cleaning_summary()
        self.logger.info(
            f"âœ… è³‡æ–™æ¸…ç†å®Œæˆ Data cleaning completed: "
            f"{cleaning_summary['final_count']:,} records "
            f"({cleaning_summary['retention_rate']:.1f}% retained)"
        )
        
        return self.cleaned_data
    
    @log_execution_time
    def calculate_rfm(self, analysis_date: Optional[str] = None) -> pd.DataFrame:
        """
        è¨ˆç®— RFM æŒ‡æ¨™
        
        Args:
            analysis_date: åˆ†ææ—¥æœŸ (YYYY-MM-DD æ ¼å¼)
            
        Returns:
            pd.DataFrame: RFM è³‡æ–™
        """
        if self.cleaned_data is None:
            raise ValueError("è«‹å…ˆæ¸…ç†è³‡æ–™ Please clean data first")
        
        self.logger.info("ğŸ¯ é–‹å§‹ RFM åˆ†æ Starting RFM analysis")
        
        # è™•ç†åˆ†ææ—¥æœŸ
        if analysis_date:
            from datetime import datetime
            analysis_date = datetime.strptime(analysis_date, "%Y-%m-%d")
        
        self.rfm_calculator = RFMCalculator(self.cleaned_data)
        self.rfm_data = self.rfm_calculator.calculate_rfm(analysis_date)
        
        self.logger.info(f"âœ… RFM è¨ˆç®—å®Œæˆ RFM calculation completed: {len(self.rfm_data):,} customers")
        return self.rfm_data
    
    @log_execution_time
    def segment_customers(self, rfm_bins: int = 5) -> pd.DataFrame:
        """
        é€²è¡Œå®¢æˆ¶åˆ†ç¾¤
        
        Args:
            rfm_bins: RFM åˆ†æ•¸çš„åˆ†çµ„æ•¸é‡
            
        Returns:
            pd.DataFrame: åˆ†ç¾¤å¾Œçš„å®¢æˆ¶è³‡æ–™
        """
        if self.rfm_data is None:
            raise ValueError("è«‹å…ˆè¨ˆç®— RFM æŒ‡æ¨™ Please calculate RFM first")
        
        self.logger.info("ğŸ‘¥ é–‹å§‹å®¢æˆ¶åˆ†ç¾¤ Starting customer segmentation")
        
        # è¨ˆç®— RFM åˆ†æ•¸
        rfm_scores = self.rfm_calculator.calculate_rfm_scores(
            r_bins=rfm_bins, f_bins=rfm_bins, m_bins=rfm_bins
        )
        
        # é€²è¡Œåˆ†ç¾¤
        self.segmented_data = self.rfm_calculator.segment_customers(rfm_scores)
        
        # è¨ˆç®—å®¢æˆ¶çµ‚èº«åƒ¹å€¼
        self.segmented_data = self.rfm_calculator.calculate_customer_lifetime_value(
            self.segmented_data
        )
        
        segment_counts = self.segmented_data['Customer_Segment'].value_counts()
        self.logger.info(f"âœ… å®¢æˆ¶åˆ†ç¾¤å®Œæˆ Customer segmentation completed: {len(segment_counts)} segments")
        
        return self.segmented_data
    
    @log_execution_time
    def create_visualizations(self, save_plots: bool = True) -> None:
        """
        å»ºç«‹è¦–è¦ºåŒ–åœ–è¡¨
        
        Args:
            save_plots: æ˜¯å¦å„²å­˜åœ–è¡¨
        """
        if self.segmented_data is None:
            raise ValueError("è«‹å…ˆå®Œæˆå®¢æˆ¶åˆ†ç¾¤ Please complete customer segmentation first")
        
        self.logger.info("ğŸ¨ é–‹å§‹å»ºç«‹è¦–è¦ºåŒ–åœ–è¡¨ Starting visualization creation")
        
        self.visualizer = DataVisualizer()
        
        # è¨­ç½®åœ–è¡¨ä¿å­˜è·¯å¾‘
        plots_dir = Path("plots")
        plots_dir.mkdir(parents=True, exist_ok=True)
        
        # å»ºç«‹å„ç¨®åœ–è¡¨
        try:
            if save_plots:
                # ä¿å­˜æ‰€æœ‰åœ–è¡¨åˆ° plots ç›®éŒ„
                self.visualizer.save_all_plots(
                    rfm_data=self.rfm_data,
                    segmented_data=self.segmented_data,
                    cleaned_data=self.cleaned_data,
                    output_dir=plots_dir
                )
            else:
                # åªé¡¯ç¤ºåœ–è¡¨ï¼Œä¸ä¿å­˜
                self.visualizer.plot_rfm_distributions(self.rfm_data)
                self.visualizer.plot_rfm_correlation(self.rfm_data)
                self.visualizer.plot_customer_segments(self.segmented_data)
                self.visualizer.plot_time_series_analysis(self.cleaned_data)
                self.visualizer.plot_geographic_analysis(self.cleaned_data)
            
            self.logger.info("âœ… è¦–è¦ºåŒ–åœ–è¡¨å»ºç«‹å®Œæˆ Visualization creation completed")
            
        except Exception as e:
            self.logger.error(f"âŒ è¦–è¦ºåŒ–å»ºç«‹å¤±æ•— Visualization creation failed: {e}")
            raise
    
    @log_execution_time
    def export_results(self, output_dir: Optional[Path] = None) -> Dict[str, Path]:
        """
        åŒ¯å‡ºåˆ†æçµæœ
        
        Args:
            output_dir: è¼¸å‡ºç›®éŒ„ï¼Œé è¨­ä½¿ç”¨é…ç½®ä¸­çš„ results_dir
            
        Returns:
            Dict[str, Path]: åŒ¯å‡ºæª”æ¡ˆçš„è·¯å¾‘å­—å…¸
        """
        if self.segmented_data is None:
            raise ValueError("è«‹å…ˆå®Œæˆåˆ†æ Please complete analysis first")
        
        if output_dir is None:
            output_dir = self.config.results_dir
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"ğŸ’¾ åŒ¯å‡ºåˆ†æçµæœ Exporting results to: {output_dir}")
        
        exported_files = {}
        
        try:
            # åŒ¯å‡ºå®Œæ•´åˆ†æçµæœ
            results_file = output_dir / "customer_segmentation_results.csv"
            self.segmented_data.to_csv(results_file, index=False, encoding=self.config.encoding)
            exported_files['segmentation_results'] = results_file
            
            # åŒ¯å‡ºæ¸…ç†å¾Œçš„è³‡æ–™åˆ° processed ç›®éŒ„
            processed_dir = self.config.processed_data_dir
            processed_dir.mkdir(parents=True, exist_ok=True)
            cleaned_file = processed_dir / "cleaned_data.csv"
            self.cleaned_data.to_csv(cleaned_file, index=False, encoding=self.config.encoding)
            exported_files['cleaned_data'] = cleaned_file
            
            # åŒ¯å‡º RFM è³‡æ–™
            rfm_file = output_dir / "rfm_data.csv"
            self.rfm_data.to_csv(rfm_file, index=False, encoding=self.config.encoding)
            exported_files['rfm_data'] = rfm_file
            
            # åŒ¯å‡ºåˆ†ç¾¤æ‘˜è¦
            segment_summary = self.rfm_calculator.get_segment_summary(self.segmented_data)
            summary_file = output_dir / "segment_summary.csv"
            segment_summary.to_csv(summary_file, encoding=self.config.encoding)
            exported_files['segment_summary'] = summary_file
            
            self.logger.info(f"âœ… çµæœåŒ¯å‡ºå®Œæˆ Results exported: {len(exported_files)} files")
            
        except Exception as e:
            self.logger.error(f"âŒ çµæœåŒ¯å‡ºå¤±æ•— Results export failed: {e}")
            raise
        
        return exported_files
    
    @log_execution_time
    def run_full_analysis(
        self,
        file_path: Union[str, Path],
        remove_outliers: bool = False,
        rfm_bins: int = 5,
        analysis_date: Optional[str] = None,
        create_plots: bool = True,
        export_results: bool = True
    ) -> Dict:
        """
        åŸ·è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        
        Args:
            file_path: è³‡æ–™æª”æ¡ˆè·¯å¾‘
            remove_outliers: æ˜¯å¦ç§»é™¤ç•°å¸¸å€¼
            rfm_bins: RFM åˆ†æ•¸çš„åˆ†çµ„æ•¸é‡
            analysis_date: åˆ†ææ—¥æœŸ
            create_plots: æ˜¯å¦å»ºç«‹è¦–è¦ºåŒ–åœ–è¡¨
            export_results: æ˜¯å¦åŒ¯å‡ºçµæœ
            
        Returns:
            Dict: åˆ†æçµæœæ‘˜è¦
        """
        self.logger.info("ğŸš€ é–‹å§‹å®Œæ•´åˆ†ææµç¨‹ Starting full analysis pipeline")
        
        try:
            # 1. è¼‰å…¥è³‡æ–™
            self.load_data(file_path)
            
            # 2. æ¸…ç†è³‡æ–™
            self.clean_data(remove_outliers=remove_outliers)
            
            # 3. è¨ˆç®— RFM
            self.calculate_rfm(analysis_date=analysis_date)
            
            # 4. å®¢æˆ¶åˆ†ç¾¤
            self.segment_customers(rfm_bins=rfm_bins)
            
            # 5. å»ºç«‹è¦–è¦ºåŒ–
            if create_plots:
                self.create_visualizations()
            
            # 6. åŒ¯å‡ºçµæœ
            exported_files = {}
            if export_results:
                exported_files = self.export_results()
            
            # ç”Ÿæˆåˆ†ææ‘˜è¦
            summary = self.generate_analysis_summary()
            summary['exported_files'] = exported_files
            
            self.logger.info("ğŸ‰ å®Œæ•´åˆ†ææµç¨‹å®Œæˆ Full analysis pipeline completed")
            return summary
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææµç¨‹å¤±æ•— Analysis pipeline failed: {e}")
            raise
    
    def generate_analysis_summary(self) -> Dict:
        """
        ç”Ÿæˆåˆ†ææ‘˜è¦å ±å‘Š
        
        Returns:
            Dict: åˆ†ææ‘˜è¦
        """
        if self.segmented_data is None:
            raise ValueError("è«‹å…ˆå®Œæˆåˆ†æ Please complete analysis first")
        
        # åŸºæœ¬çµ±è¨ˆ
        cleaning_summary = self.cleaner.get_cleaning_summary()
        segment_summary = self.rfm_calculator.get_segment_summary(self.segmented_data)
        
        summary = {
            'data_overview': {
                'original_records': cleaning_summary['original_count'],
                'cleaned_records': cleaning_summary['final_count'],
                'retention_rate': cleaning_summary['retention_rate'],
                'customers_analyzed': len(self.rfm_data),
                'date_range': {
                    'start': self.cleaned_data['InvoiceDate'].min().date().isoformat(),
                    'end': self.cleaned_data['InvoiceDate'].max().date().isoformat()
                }
            },
            'rfm_statistics': {
                'avg_recency': float(self.rfm_data['Recency'].mean()),
                'avg_frequency': float(self.rfm_data['Frequency'].mean()),
                'avg_monetary': float(self.rfm_data['Monetary'].mean()),
                'total_revenue': float(self.rfm_data['Monetary'].sum())
            },
            'segmentation_results': {
                'total_segments': self.segmented_data['Customer_Segment'].nunique(),
                'top_segments': segment_summary.head(3).to_dict('index')
            }
        }
        
        return summary
    
    def get_business_insights(self) -> Dict:
        """
        ç²å–æ¥­å‹™æ´å¯Ÿ
        
        Returns:
            Dict: æ¥­å‹™æ´å¯Ÿå’Œå»ºè­°
        """
        if self.segmented_data is None:
            raise ValueError("è«‹å…ˆå®Œæˆåˆ†æ Please complete analysis first")
        
        insights = {}
        
        # é«˜åƒ¹å€¼å®¢æˆ¶åˆ†æ
        champions = self.segmented_data[
            self.segmented_data['Customer_Segment'] == 'Champions'
        ]
        if len(champions) > 0:
            champions_revenue = champions['Monetary'].sum()
            champions_percentage = len(champions) / len(self.segmented_data) * 100
            insights['champions'] = {
                'count': len(champions),
                'percentage': champions_percentage,
                'revenue_contribution': champions_revenue,
                'avg_clv': champions['CLV'].mean() if 'CLV' in champions.columns else None
            }
        
        # é¢¨éšªå®¢æˆ¶åˆ†æ
        at_risk = self.segmented_data[
            self.segmented_data['Customer_Segment'].isin(['At Risk', 'Cannot Lose Them'])
        ]
        if len(at_risk) > 0:
            insights['at_risk'] = {
                'count': len(at_risk),
                'percentage': len(at_risk) / len(self.segmented_data) * 100,
                'potential_loss': at_risk['Monetary'].sum()
            }
        
        return insights
